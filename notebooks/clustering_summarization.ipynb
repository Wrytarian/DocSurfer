{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.cluster import KMeansClusterer, euclidean_distance, cosine_distance\n",
    "from gensim.models import Word2Vec\n",
    "import re\n",
    "import urllib\n",
    "import bs4 as bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get text from wiki article\n",
    "data = urllib.request.urlopen(f'https://en.wikipedia.org/wiki/Basketball')\n",
    "article = data.read()\n",
    "parsed_article = bs.BeautifulSoup(article,'lxml')\n",
    "paragraphs = parsed_article.find_all('p')\n",
    "text = \"\"\n",
    "\n",
    "for p in paragraphs:\n",
    "    text += p.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do some processing\n",
    "text = text[2:-1].replace(\"\\\\r\\\\n\", \" \")\n",
    "text = text.replace(\"\\\\n\", \" \")\n",
    "text = text.replace(\"\\\\x0c\", \" \")\n",
    "text = ' '.join(text.split()).strip()\n",
    "text = re.sub(r'\\[[0-9]*\\]', ' ', text)\n",
    "text = re.sub(r'\\s+', ' ', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split text into sentences\n",
    "sentences = sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a cleaned version of the text\n",
    "cleaned_txt = []\n",
    "for i in range(len(sentences)):\n",
    "    sen = re.sub('[^a-zA-Z]', \" \", sentences[i])  \n",
    "    sen = sen.lower()                            \n",
    "    sen = sen.split()                         \n",
    "    sen = ' '.join([i for i in sen if i not in stopwords.words('english')])   \n",
    "    cleaned_txt.append(sen)\n",
    "    \n",
    "all_words = [i.split() for i in cleaned_txt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(all_words, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vectors from sentences\n",
    "sent_vector=[]\n",
    "for i in cleaned_txt:\n",
    "    plus=0\n",
    "    for j in i.split():\n",
    "        plus+= model.wv[j]\n",
    "    plus = plus/len(i.split())\n",
    "    \n",
    "    sent_vector.append(plus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance = \"euclidean\"\n",
    "if distance == \"cosine\":\n",
    "    distance = cosine_distance\n",
    "elif distance == \"euclidean\":\n",
    "    distance = euclidean_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize clusterer\n",
    "n_clusters = 8\n",
    "kclusterer = KMeansClusterer(num_means=n_clusters, distance=distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get clusters for sentence vectors and the centroids of the clusters\n",
    "clusters = kclusterer.cluster(sent_vector, True)    # Cluster indices for each sentence\n",
    "means = kclusterer.means()  # Vector for each centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "\n",
    "summary_indices = []  # List of sentence indices for summary\n",
    "for cluster_index in range(n_clusters):\n",
    "    distances = {}\n",
    "    for j in range(len(clusters)):  # Loop through the sentence clusters\n",
    "        if clusters[j] == cluster_index:\n",
    "            # Calculate the distance between the cluster's centroid and the sentence vector\n",
    "            distances[j] = distance.euclidean(means[cluster_index], sent_vector[j])\n",
    "\n",
    "    # Add index of the sentence closest to the cluster's centroid to the summary indices list\n",
    "    summary_indices.append(min(distances, key=distances.get))\n",
    "\n",
    "summary = []\n",
    "for i in summary_indices:\n",
    "    summary.append(sentences[i])\n",
    "summary = \" \".join(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(summary)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b55fb9c3b2ae460387b9f24a585a06d07a1c685c2da458447f8c89d29042cf89"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('venv': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
