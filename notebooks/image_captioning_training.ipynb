{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.applications import InceptionV3\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from glob import glob\n",
    "import PIL\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"../datasets/Flickr8k/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = dataset_path + \"Images/\"\n",
    "images = glob(image_path + \"*.jpg\")\n",
    "len(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in np.random.randint(0, len(images), size=3):\n",
    "    plt.figure()\n",
    "    image = cv2.imread(images[i])\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        text = file.read()\n",
    "    return text\n",
    "\n",
    "file = dataset_path + \"captions_removed_first_and_last.txt\"\n",
    "info = load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in info.split(\"\\n\")[:3]:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_captions(info):\n",
    "    data = dict()\n",
    "    for line in info.split('\\n'):\n",
    "        img, caption = line.split('.jpg,')        \n",
    "        if img not in data:\n",
    "            data[img] = []\n",
    "        data[img].append(caption)\n",
    "        \n",
    "    return data\n",
    "\n",
    "data = load_captions(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(data.items())[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanse_data(data):\n",
    "    clean_data = dict()\n",
    "\n",
    "    for img, captions in data.items():\n",
    "        for i in range(len(captions)):\n",
    "            caption = captions[i]\n",
    "            lower_words = [word.lower() for word in caption.split() if len(word) > 1]\n",
    "            clean_caption = \" \".join(lower_words)\n",
    "\n",
    "            if img not in clean_data:\n",
    "                clean_data[img] = []\n",
    "            \n",
    "            clean_data[img].append(clean_caption)\n",
    "            \n",
    "    return clean_data\n",
    "\n",
    "clean_data = cleanse_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(clean_data.items())[50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_words(data):\n",
    "    all_words = set()\n",
    "    for img_key in data.keys():\n",
    "        for caption in data[img_key]:\n",
    "                all_words.update(caption.split())\n",
    "    return all_words\n",
    "\n",
    "# summarize vocabulary\n",
    "all_words = get_all_words(clean_data)\n",
    "print(len(all_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data(data, filename):\n",
    "    lines = list()\n",
    "    for img_key, captions in data.items():\n",
    "        for caption in captions:\n",
    "            lines.append(img_key + ' ' + caption)\n",
    "    data = '\\n'.join(lines)\n",
    "\n",
    "    with open(filename, 'w') as file:\n",
    "        file.write(data)\n",
    "\n",
    "save_data(clean_data, dataset_path + 'captions_clean.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = dataset_path + 'Images/'\n",
    "img_paths = glob(images + '*.jpg')\n",
    "print(len(img_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_img(image_path):\n",
    "    # Convert all the images to size 299x299 as expected by the inception v3 model\n",
    "    img = keras.preprocessing.image.load_img(image_path, target_size=(299, 299))\n",
    "    # Convert PIL image to numpy array of 3-dimensions\n",
    "    x = keras.preprocessing.image.img_to_array(img)\n",
    "    # Add one more dimension\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    # preprocess the images using preprocess_input() from inception module\n",
    "    x = keras.applications.inception_v3.preprocess_input(x)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the inception v3 model\n",
    "input1 = InceptionV3(weights='imagenet')\n",
    "\n",
    "# Create a new model, by removing the last layer (output layer) from the inception v3\n",
    "model = Model(input1.input, input1.layers[-2].output)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to encode a given image into a vector of size (2048, )\n",
    "def encode(image):\n",
    "    image = preprocess_img(image) # preprocess the image\n",
    "    feature_vec = model.predict(image) # Get the encoding vector for the image\n",
    "    feature_vec = np.reshape(feature_vec, feature_vec.shape[1]) # reshape from (1, 2048) to (2048, )\n",
    "    return feature_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = {}\n",
    "\n",
    "for img_path in tqdm(img_paths):\n",
    "    encoding[img_path[len(images):]] = encode(img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the features in the images1 pickle file\n",
    "with open(dataset_path + \"images_features.pkl\", \"wb\") as encoded_pickle:\n",
    "    pickle.dump(encoding, encoded_pickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(dataset_path + \"images_features.pkl\", \"rb\") as encoded_pickle:\n",
    "    encoding = pickle.load(encoded_pickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_captions = []\n",
    "\n",
    "for img_key, captions in clean_data.items():\n",
    "    for cap in captions:\n",
    "        training_captions.append(cap)\n",
    "        \n",
    "len(training_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count_threshold = 5\n",
    "word_counts = {}\n",
    "\n",
    "for cap in training_captions:\n",
    "    for w in cap.split(' '):\n",
    "        word_counts[w] = word_counts.get(w, 0) + 1\n",
    "\n",
    "all_words = [w for w in word_counts if word_counts[w] >= word_count_threshold]\n",
    "print(f'preprocessed words {len(word_counts)} -> {len(all_words)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ix_to_word = {}\n",
    "word_to_ix = {}\n",
    "\n",
    "for ix, w in enumerate(all_words):\n",
    "    word_to_ix[w] = ix\n",
    "    ix_to_word[ix] = w\n",
    "    \n",
    "vocab_size = len(ix_to_word) + 1 # one for appended 0's\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(dataset_path + \"word_to_ix.pkl\", \"wb\") as encoded_pickle:\n",
    "    pickle.dump(word_to_ix, encoded_pickle)\n",
    "    \n",
    "with open(dataset_path + \"ix_to_word.pkl\", \"wb\") as encoded_pickle:\n",
    "    pickle.dump(ix_to_word, encoded_pickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_lines(data):\n",
    "    captions = []\n",
    "    for img_key in data.keys():\n",
    "        for caption in data[img_key]:\n",
    "            captions.append(caption)\n",
    "    return captions\n",
    "\n",
    "# calculate the length of the description with the most words\n",
    "def max_length(data):\n",
    "    lines = to_lines(data)\n",
    "    return max(len(d.split()) for d in lines)\n",
    "\n",
    "# determine the maximum sequence length\n",
    "max_length = max_length(clean_data)\n",
    "print(f'Description Length: {max_length}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(descriptions, photos, wordtoix, max_length, num_photos_per_batch):\n",
    "    X1, X2, y = list(), list(), list()\n",
    "    n=0\n",
    "    # loop for ever over images\n",
    "    while True:\n",
    "        for key, desc_list in descriptions.items():\n",
    "            n += 1\n",
    "            # retrieve the photo feature\n",
    "            photo = photos[key+'.jpg']\n",
    "            for desc in desc_list:\n",
    "                # encode the sequence\n",
    "                seq = [wordtoix[word] for word in desc.split(' ') if word in wordtoix]\n",
    "                # split one sequence into multiple X, y pairs\n",
    "                for i in range(1, len(seq)):\n",
    "                    # split into input and output pair\n",
    "                    in_seq, out_seq = seq[:i], seq[i]\n",
    "                    # pad input sequence\n",
    "                    in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "                    # encode output sequence\n",
    "                    out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "                    # store\n",
    "                    X1.append(photo)\n",
    "                    X2.append(in_seq)\n",
    "                    y.append(out_seq)\n",
    "            # yield the batch data\n",
    "            if n==num_photos_per_batch:\n",
    "                yield ([array(X1), array(X2)], array(y))\n",
    "                X1, X2, y = list(), list(), list()\n",
    "                n=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = {} # empty dictionary\n",
    "\n",
    "with open('../datasets/glove.840B.300d.txt', encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        try:\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "        except Exception:\n",
    "            continue\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print(f'Found {len(embeddings_index)} word vectors.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 300\n",
    "\n",
    "# Get 200-dim dense vector for each of the words in our vocabulary\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "num_found = 0\n",
    "for word, i in word_to_ix.items():\n",
    "    #if i < max_words:\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in the embedding index will be all zeros,\n",
    "        num_found += 1\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "print(f'{len(all_words) - num_found} words not found.')\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Input, Conv2D, MaxPool2D, LSTM, add\n",
    "from tensorflow.keras.layers import Activation, Dropout, Flatten, Embedding\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "inputs1 = Input(shape=(2048,))\n",
    "fe1 = Dropout(0.5)(inputs1)\n",
    "fe2 = Dense(256, activation='relu')(fe1)\n",
    "inputs2 = Input(shape=(max_length,))\n",
    "se1 = Embedding(vocab_size, embedding_dim, mask_zero=True)(inputs2)\n",
    "se2 = Dropout(0.5)(se1)\n",
    "se3 = LSTM(256)(se2)\n",
    "decoder1 = add([fe2, se3])\n",
    "decoder2 = Dense(256, activation='relu')(decoder1)\n",
    "outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
    "model = Model(inputs=[inputs1, inputs2], outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[2].set_weights([embedding_matrix])\n",
    "model.layers[2].trainable = False\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "number_pics_per_bath = 3\n",
    "steps = len(clean_data)//number_pics_per_bath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(dataset_path + \"images_features.pkl\", \"rb\") as p:\n",
    "    features = pickle.load(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.run_functions_eagerly(True)\n",
    "for i in range(epochs):\n",
    "    generator = data_generator(clean_data, features, word_to_ix, max_length, number_pics_per_bath)\n",
    "    model.fit(generator, epochs=1, steps_per_epoch=steps, verbose=1)\n",
    "    model.save('model_' + str(i) + '.h5')\n",
    "model.save_weights('./model_weights/final_model.h5')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b55fb9c3b2ae460387b9f24a585a06d07a1c685c2da458447f8c89d29042cf89"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('venv': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
